
# 随机森林
**约定：**
- <font color=F66A65>红色：</font>对[readme.md](./readme.md)中提到的名词进行解释的重点
- <font color=FDA63E>黄色：</font>各部分总结中的重点
- <font color=62D257>绿色：</font>算法基本假设与解释

**摘要：**
1. 随机森林的两部分【重点是「随机」的部分】（[随机](#21-随机)、[森林](#22-森林)）
1. 集成学习+随机带来的额外的评价方法（[OOB](#三算法评价（OOB）)）

**名词解释：**``集成学习`、`Bagging/Stacking/Boosting`、`OOB`
> 1. 集成学习：表示一类算法，该算法并不强调某种算法，而是某种思想：<font color=F66A65>通过将多个弱分类器组合成为一个较强的分类器</font>
> 2. Bagging/Stacking/Boosting：三种集成学习的「组装」思想，通过将<font color=F66A65>弱分类器怎样的架构排列得到最终的结果</font>的三种方法
> 3. OOB：Out of Bag，对于Bagging算法特有的一种<font color=F66A65>选取样本做测试的方法</font>（选那些未被选取作为训练数据的样本）给定一定的信息后熵的大小，和“后验概率”的思想有点像
* 注：Bagging、Stacking和Boosting都是集成学习中组合若分类器的一种方式在[番外-集成学习的讨论](./番外-集成学习的讨论（Bagging、Stacking、Boosting）.md)中详细解释了三种思想的对比。

**目录：**

----

Bagging: 随机森林 == CART加两种随机
    内容
        增加限定的bagging：只能是决策树
        样本随机选：有放回的抽样（n） boostrap
        特征随机选：k个  k一般是 log2（总特征个数）
        重复m次，得到m个CART（不剪枝），投票
    OOB包外估计：效率高无偏估计，近似于k折交叉验证(3/7开)
        以树为单位的计算
            树的OOB误分率：每棵树没有选中的哪些样本的错误分类
            随机森林的误分率：所有树误分率的平均
        以样本为单位的计算
            对任何一个样本来说，有2/3的树选了它，1/3的树没选它
            没选它的那 1/3 对这个样本做分类
    优势
        不易过拟合，抗噪声，连续/离散

## 一、一句话概括


## 二、随机森林
### 2.1 随机

### 2.2 森林

### 2.3 算法过程

## 三、算法评价（OOB）

## 四、总结

